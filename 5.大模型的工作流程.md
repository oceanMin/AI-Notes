# **大模型的工作流程**

## 一、分词化（Tokenization）与词表映射  

### 1. 什么是分词化？  
- 将段落或句子分割成更小的分词（token）的过程。  
- 是自然语言处理（NLP）中的重要步骤，帮助计算机理解句子结构和上下文。

### 2. 分词化示例  
- **英文句子**：`I want to study ACA.`  
- **分词化结果**：  
  ```text
  ['I', 'want', 'to', 'study', 'ACA', '.']
  ```

### 3. 分词粒度分类  
| 粒度类型 | 说明 | 适用场景 |
|----------|------|----------|
| **词粒度** | 以单词为单位分词 | 英语等西方语言 |
| **字符粒度** | 以单个字符/汉字为单位分词 | 中文等表意文字 |
| **子词粒度** | 将单词分解为词根、词缀等子词 | 处理新词、专有名词、网络用语 |

### 4. 词表映射（Token IDs）  
- 每个 token 通过预设词典映射为一个唯一的 **Token ID**（类似“身份证”）。  
- 一句话最终表示为 Token ID 列表，供计算机处理。

#### 示例：
| Token        | Token ID |
|--------------|----------|
| realty       | 41       |
| enjoy        | 2787     |
| learning     | 4669     |
| ACA          | 4812     |
| certification| 6433     |
| ...          | ...      |

---

## 二、大语言模型生成文本的过程  

### 1. 核心机制：自回归预测  
- 大语言模型根据给定的文本（提示词）预测下一个最可能的 token。  
- 过程不是一步到位，而是逐步迭代生成。

### 2. 生成步骤  
1. **输入提示文本** → 模型基于现有 token 计算下一个 token 的概率分布。  
2. **选择下一个 token** → 通常按概率最大原则选取（也可引入随机性）。  
3. **将新 token 加入输入序列** → 更新输入文本。  
4. **重复预测** → 继续预测下一个 token，直至生成结束标志或达到长度上限。  
5. **结束标志** → 如 `<EOS>`（End of Sentence）或预设长度阈值。

### 3. 生成示例  
假设提示为：  
> “ACA is a very informative”

模型预测下一个 token 的概率可能如下：

| Token        | 概率   |
|--------------|--------|
| informative  | 50%    |
| fun          | 25%    |
| enlightening | 20%    |
| boring       | 5%     |

若选择 **informative**，则新输入为：  
> “ACA is a very informative”

继续预测下一个 token：

| Token    | 概率   |
|----------|--------|
| course   | 73%    |
| subject  | 14%    |
| journey  | 10%    |
| manner   | 3%     →

最终可能生成：  
> “ACA is a very informative course.”

---

## 三、流程总结  
1. **输入文本** → 分词化 → 映射为 Token IDs  
2. **模型接收 Token IDs** → 进行自回归预测  
3. **逐步生成** → 选择下一个 token → 加入序列 → 继续预测  
4. **输出生成文本** → 直至遇到结束标志或达到长度限制  
