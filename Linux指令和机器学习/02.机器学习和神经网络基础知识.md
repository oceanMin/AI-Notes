# 机器学习和神经网络基础

## 一、人工智能与机器学习体系

```
人工智能 (AI)
└── 机器学习 (ML)
    └── 深度学习 (DL)
```

### 定义与关系：
- **AI**：使计算机模拟人类智能行为的科学（学习、推理、自我改进）
- **ML**：AI的一个分支，通过数据和算法自动学习并改进性能
- **DL**：ML的一种方法，使用复杂神经网络处理大量数据，实现高级模式识别与预测

---

## 二、深度学习基础

### 核心特点：
- ✅ 神经网络模拟大脑结构与功能
- ✅ 广泛应用于图像、语音、自然语言处理
- ✅ 需要大量数据训练（优势也是挑战）

### 网络结构示例：
```
输入层 (ℝ⁷) → 隐藏层 (ℝ⁹) → 隐藏层 (ℝ⁵) → 输出层 (ℝ¹)
```

### 基本公式：
```
Z = WX + b
```
- `X`：输入特征（如光照强度）
- `W`：权重
- `b`：偏置
- `Z`：用于判断输出（如分类阈值0.5）

---

## 三、神经网络基础概念

### 1. 神经元与网络
- 输入层 → 隐藏层 → 输出层
- 前向传播：计算预测值
- 反向传播：调整参数，最小化误差

### 2. 常见网络类型：

#### （1）卷积神经网络（CNN）
- **特点**：卷积运算，局部感知，参数共享
- **应用**：图像分类、识别、视频分析
- **优势**：自适应强，泛化能力好

#### （2）循环神经网络（RNN）
- **特点**：序列建模，具有时间记忆
- **结构**：输入层、隐藏层、输出层（含循环连接）
- **应用**：语音识别、自然语言处理、时间序列预测

---

## 四、自然语言处理（NLP）与Transformer

### 1. NLP 概述
- **目标**：让计算机理解、解释、生成人类语言
- **应用**：
  - 机器翻译
  - 语音识别
  - 情感分析
  - 文本摘要
  - 聊天机器人

### 2. Transformer 架构
```
Encoder（编码器）:
  输入 → Embedding → Positional Encoding → Multi-head Attention → Add & Norm → FFN → 输出

Decoder（解码器）:
  目标序列 → Masked Multi-head Attention → Multi-head Attention（连接Encoder输出）→ FFN → 输出
```

### 3. 词嵌入（Embedding）
- 将词语映射为固定维度的向量
- 示例（7维）：
  - man: [0.6, -0.2, 0.8, 0.9, -0.1, -0.9, -0.7]
  - woman: [0.7, 0.3, 0.9, -0.7, 0.1, -0.5, -0.4]
  - king/queen 具有类似语义结构
- 可通过降维（如PCA/t-SNE）可视化词语关系

---

## 五、学习建议

### 推荐学习路径：
1. 理解基本概念（AI → ML → DL）
2. 掌握神经网络基础（前向/反向传播）
3. 学习典型网络结构（CNN、RNN）
4. 深入NLP与Transformer
5. 结合实战项目（如图像分类、文本生成）

### 实践工具建议：
- Python + TensorFlow/PyTorch
- 使用公开数据集（MNIST、CIFAR-10、IMDB等）
- 从简单模型开始，逐步增加复杂度

---
