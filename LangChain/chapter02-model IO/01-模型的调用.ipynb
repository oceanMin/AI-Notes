{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 1. 模型调用的分类\n",
    "\n",
    "角度1：按照模型功能的不同\n",
    "\n",
    "非对话模型：（LLMs、Text Model）\n",
    "\n",
    "对话模型：（Chat Models）  （推荐）\n",
    "\n",
    "嵌入模型：（Embedding Models）\n",
    "\n",
    "\n",
    "角度2：按照模型调用时，参数书写的位置不同（api-key、base_url、model_name）\n",
    "\n",
    "硬编码的方式：将参数书写在代码中\n",
    "\n",
    "使用环境变量的方式\n",
    "\n",
    "使用配置文件的方式（推荐）\n",
    "\n",
    "\n",
    "角度3：具体API的调用\n",
    "\n",
    "使用LangChain提供的API（推荐）\n",
    "\n",
    "使用OpenAI官方的API\n",
    "\n",
    "使用其他平台提供的API\n",
    "\n",
    "# 2、角度1：非对话模型的调用"
   ],
   "id": "81f2d16e3775551e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from tempfile import template\n",
    "\n",
    "# 安装必要的包（如果还没安装）\n",
    "# !pip install langchain-ollama\n",
    "\n",
    "from langchain_ollama import OllamaLLM  # 替换 OpenAI\n",
    "# 不再需要 dotenv 和 os.environ 设置 API Key\n",
    "\n",
    "# 创建本地 Ollama 模型实例\n",
    "llm = OllamaLLM(\n",
    "    model=\"deepseek-r1:7b\",  # 使用你已有的模型\n",
    "    # 或使用其他模型：\"llama3.2\", \"qwen2.5:7b\" 等\n",
    "    temperature=0.7,  # 创造性，0-1\n",
    "    num_predict=512   # 最大生成长度\n",
    ")\n",
    "\n",
    "# 调用模型\n",
    "str = llm.invoke('请写一首关于春天的诗')\n",
    "print(str)"
   ],
   "id": "75ee00473dfc6589",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2、角度1：对话模型的调用",
   "id": "3e60011d16c7d01f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY1\")\n",
    "os.environ[\"OPENAI_BASE_URL\"] = os.getenv(\"OPENAI_BASE_URL\")\n",
    "########核心代码############\n",
    "chat_model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "messages = [\n",
    " SystemMessage(content=\"我是人工智能助手，我叫小智\"),\n",
    " HumanMessage(content=\"你好，我是小明，很高兴认识你\")\n",
    "]\n",
    "response = chat_model.invoke(messages) # 输入消息列表\n",
    "print(type(response)) # <class 'langchain_core.messages.ai.AIMessage'>\n",
    "print(response.content)\n"
   ],
   "id": "e60ef7d8e5f00d9b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2、角度1：嵌入模型的调用",
   "id": "55508e6cb6d12b2d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 安装必要的包（如果还没安装）\n",
    "# !pip install langchain-community\n",
    "\n",
    "from langchain_community.embeddings import OllamaEmbeddings  # 替换 OpenAIEmbeddings\n",
    "# 不再需要 dotenv 和 API Key\n",
    "\n",
    "# 创建本地 Ollama embeddings 模型\n",
    "embeddings_model = OllamaEmbeddings(\n",
    "    model=\"llama3.2\"  # 或其他模型：\"deepseek-r1:7b\", \"nomic-embed-text\" 等\n",
    ")\n",
    "\n",
    "# 生成嵌入向量\n",
    "res1 = embeddings_model.embed_query('我是文档中的数据')\n",
    "print(f\"向量维度: {len(res1)}\")\n",
    "print(f\"前10个值: {res1[:10]}\")  # 只打印前10个值，否则太长"
   ],
   "id": "3620fb5be935bcc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 3、角度额2\n",
    "\n",
    "## 1.硬编码的方式\n",
    "\n",
    "以对话模型为例："
   ],
   "id": "96ab5a494749016d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 1、获取大模型\n",
    "chat_model = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    api_key=\"sk-I8eV20Ba0WFknZpli3ibGuqy9sXq9y5QD3x5RVgnLeBhAvzd\",\n",
    "    base_url=\"https://api.openai-proxy.org/v1\"\n",
    ")\n",
    "\n",
    "# 2、调用大模型\n",
    "response = chat_model.invoke('请介绍一下你自己?')\n",
    "\n",
    "# 3、输出响应文本\n",
    "print(response.content)"
   ],
   "id": "521a4d1c6ee76cee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3.2 使用环境变量的方式",
   "id": "66b57a2ec559b731"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 1、获取大模型\n",
    "chat_model = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    base_url=os.environ[\"OPENAI_BASE_URL\"],\n",
    ")\n",
    "\n",
    "# 2、调用大模型\n",
    "response = chat_model.invoke('请介绍一下你自己?')\n",
    "\n",
    "# 3、输出响应文本\n",
    "print(response.content)"
   ],
   "id": "ddaad8bd3b60bad9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 3.3 使用配置文件的方式\n",
    "\n",
    "创建.env的配置文件\n",
    "\n",
    "方式1："
   ],
   "id": "427ea961925b3672"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "import os\n",
    "import dotenv\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY1\")\n",
    "# os.environ[\"OPENAI_BASE_URL\"] = os.getenv(\"OPENAI_BASE_URL\")\n",
    "\n",
    "\n",
    "# 1、获取大模型\n",
    "chat_model = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY1\"),\n",
    "    base_url=os.getenv(\"OPENAI_BASE_URL\"),\n",
    ")\n",
    "\n",
    "# 2、调用大模型\n",
    "response = chat_model.invoke('什么是langchain?')\n",
    "\n",
    "# 3、输出响应文本\n",
    "print(response.content)"
   ],
   "id": "c55edb13b68186f6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "方式2：",
   "id": "97ffe5d5a9b3bcb5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "import os\n",
    "import dotenv\n",
    "\n",
    "# 加载.env配置文件\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY1\")\n",
    "os.environ[\"OPENAI_BASE_URL\"] = os.getenv(\"OPENAI_BASE_URL\")\n",
    "\n",
    "\n",
    "# 1、获取大模型\n",
    "chat_model = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\", # 默认使用的是gpt-3.5-turbo模型\n",
    "    #当没有显示的声明base_url和api_key的时候，默认会从环境变量中查找\n",
    "    max_tokens=500, #最大token数\n",
    "    temperature=0.7,    #温度，控制文本生成的“随机性”取值范围0~1，\n",
    ")\n",
    "\n",
    "# 2、调用大模型\n",
    "response = chat_model.invoke('什么是langchain?')\n",
    "\n",
    "# 3、输出响应文本\n",
    "print(response.content)"
   ],
   "id": "a15a84d8780ba4f6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY1\")\n",
    "os.environ[\"OPENAI_BASE_URL\"] = os.getenv(\"OPENAI_BASE_URL\")\n",
    "\n",
    "messages = [SystemMessage(content=\"你是一位哲学家\"),\n",
    "            HumanMessage(content=\"你好，请介绍一下你自己，包括自己名字\"),\n",
    "            AIMessage(content=\"我叫小智，能帮你解决各种哲学问题\")]\n",
    "\n",
    "chat_model = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    max_tokens=500,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "response = chat_model.invoke(messages)\n",
    "\n",
    "print(response.content)\n",
    "print(messages)"
   ],
   "id": "5ef44aef78a15e2d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 演示硅基流动模型调用",
   "id": "6d7328abc706e9ed"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-08T09:11:30.676737200Z",
     "start_time": "2026-02-08T09:11:26.197915800Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "# 最佳实践：从环境变量读取API密钥，避免密钥泄露\n",
    "# 请在本地设置环境变量 SILICONFLOW_API_KEY\n",
    "api_key = os.getenv(\"SILICONFLOW_API_KEY\", \"sk-xxsjtyvrotumzxejpnrfwuwuktogndgxggxqhwgiwpjlurce\")  # 临时测试可替换，但长期务必用环境变量\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=api_key,  # 使用新密钥\n",
    "    base_url=\"https://api.siliconflow.cn/v1\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"Qwen/Qwen3-8B\",  # 确认免费的模型\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": \"你好，请介绍一下你自己\"}\n",
    "        ]\n",
    "    )\n",
    "    print(\"调用成功！回复如下：\")\n",
    "    print(response.choices[0].message.content)\n",
    "except Exception as e:\n",
    "    print(f\"调用失败，错误详情：{e}\")\n",
    "    # 如果是OpenAI库的错误，通常会有更详细的响应体\n",
    "    if hasattr(e, 'response'):\n",
    "        print(f\"HTTP状态码: {e.response.status_code}\")\n",
    "        print(f\"错误信息: {e.response.text}\")"
   ],
   "id": "7458656d46140b8c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "调用成功！回复如下：\n",
      "你好！我是通义千问，通义实验室推出的超大规模语言模型，我的英文名叫Qwen。我是一种基于人工智能技术的自然语言处理模型，能够理解和生成人类语言。我的训练数据来自于互联网上的大量文本，包括书籍、文章、网页内容等，因此我具备广泛的知识和语言理解能力。\n",
      "\n",
      "我可以回答各种问题，从科技、文化到日常生活，几乎无所不能。同时，我也可以进行创作，比如写故事、诗歌，甚至编程。我还能进行多轮对话，记住上下文，从而提供更连贯和个性化的回应。\n",
      "\n",
      "我的目标是为用户提供有用、准确和友好的帮助，无论是学习、工作还是娱乐。如果你有任何问题或需要帮助的地方，随时告诉我，我会尽力为你解答！\n"
     ]
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
